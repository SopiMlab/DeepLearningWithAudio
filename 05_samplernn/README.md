# SampleRNN

SampleRNN is another generative deep learning model for audio, introduced in 2017 by Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville and Yoshua Bengio. It trains on an dataset of unlabeled audio chunks and learns to generate sequences of similar audio. The architecture of SampleRNN is based on recurrent neural networks (RNNs), in which the output of each time step is fed back as input to the next one. RNNs are well suited for modeling sequential data, but suffer from the vanishing gradient problem, in which the network's ability to learn is hindered by its gradients shrinking exponentially during backpropagation. Two popular variants of RNN have been developed to address the vanishing gradient problem: gated recurrent units (GRU) and long short-term memory (LSTM). SampleRNN can be configured to work with either variant, and it's not currently clear whether either one is universally better.

The use of RNNs allows SampleRNN to generate audio sequences of any duration. This is in contrast to the fixed-length sequences of GANSynth, however SampleRNN is significantly slower. As the [original implementation of SampleRNN](https://github.com/soroushmehr/sampleRNN_ICLR2017) is unmaintained and hard to set up successfully, we use [our own fork](https://github.com/SopiMlab/prism-samplernn/) of the [PRiSM SampleRNN](https://github.com/rncm-prism/prism-samplernn/) implementation.

The character of the audio generated by SampleRNN can be influenced by a parameter known as the sampling temperature. It is also possible to use a "seed" audio as input to affect the output, but at the time of writing, this is [not working as intended](https://github.com/rncm-prism/prism-samplernn/issues/9#issuecomment-760905603).

SampleRNN has been extensively used by the group [DADABOTS](https://dadabots.com) to generate entire albums in various styles including death metal and free jazz.

## Setup (macOS/Linux)

First make sure you have [pyext](../utilities/pyext-setup/) set up.

To avoid potential version incompatibilities between SampleRNN and our existing Conda environment for Magenta, we will create a separate one for SampleRNN.

Create the environment:

```
conda create -n samplernn python=3.8 tensorflow=2
```

If you're on Linux and have an NVIDIA GPU with CUDA support, you can use `tensorflow-gpu` instead for much better performance:

```
conda create -n samplernn python=3.8 tensorflow-gpu=2
```

Activate the environment:

```
conda activate samplernn
```

Enter the samplernn directory in the course repository (in this example, located in the home directory):

```
cd ~/DeepLearningWithAudio/05_samplernn
```

Clone our PRiSM SampleRNN repository:

```
git clone https://github.com/SopiMlab/prism-samplernn
```

Enter the resulting directory:

```
cd prism-samplernn
```

Install SampleRNN:

```
pip install -e .
```

Install the sopilib support library from the course repository's root:

```
pip install -e ../utilities/sopilib
```

Find out the path of your Python interpreter:

```
which python
```

Example output (your path may differ):

```
/usr/local/Caskroom/miniconda/base/envs/ddsp/bin/python
```

Finally, open `samplernn.pd`. You will need to edit the `load` message to use your Python executable path.

## Checkpoints

TODO

## Exercises

1. Try generating some sounds with different values for the sampling temperature parameter. How does it affect the results?

## Links

- [SampleRNN: An Unconditional End-to-End Neural Audio Generation Model](https://arxiv.org/abs/1612.07837)